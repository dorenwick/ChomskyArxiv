Here is the outline for our strategy for reading through Chomskys work.

We need to first identify how to remove duplicates
of chomsky's works, and use title search to find which of our downloads are actually written by him.
We shall also need to figure out whether to prioritize epub or pdf versions, or mobi.
I think we will prioritize epub/mobi, and any that are pdf's from him shall be cleaned via adobe, so
we can extract paragraphs from them. (this is the hope!).

we can use grobid on the pdfs as well, which shall also be useful.
we will do that with chapters.

Now, we will describe our schema.
we want to use snowflake-xs to encode paragraph text, and sentence text too (possibly).

then, we have a structured schema in nosql database that will contain the following fields:


paragraph_int_id: (integer, automatically should increment)
type: (book, youtube, article, interview, debate, talk,...)
source: (name of book title, youtube video title, article title, interview, debate, talk, ect )
url: (chomsky.info link, youtube url, book url if theres an open url).
timestamp: (only applies if its a video)
location: (either a page number, or % of the way through the article, youtube video, book, ect.)
text: (the actual text of the paragraph)
keyphrases: [] (list of keyphrases we have)
keynames:  [] (list of people associated or talked about)
questions: [] (list of questions appearing in the article text).
answers: [] (list of answers appearing in the article text. we may have a question_id and answer_id to say which questions are being answered.)
topic: (topic that chomsky is talking about. we may use something like BERTopic to create our own topics, and topic classifier here).

database is ChomskyArxiv
Collection is called ChomskyArchive
mongodb connection is mongodb_url="mongodb://localhost:27017/",


We want to start processing the youtube videos first.
We will have to extract the type (easy, it will be build into our pipeline)
We will have to extract the source (easy, we use youtube url files.

C:\Users\doren\PycharmProjects\ChomskyArchive\web_data\youtube_urls.json

{
  "youtube_urls": [
    "http://www.youtube.com/results?search_query=noam+chomsky",
    "https://www.youtube.com/watch?v=eRLLRzmbk6g",
    "https://www.youtube.com/watch?v=8aDjX54nmJY",
    "https://www.youtube.com/watch?v=Sk2pVd9Wdiw",
    "https://www.youtube.com/watch?v=T-IZY0VUQ0s",
    "https://www.youtube.com/watch?v=TQ-Crh3rdQA",
    "https://www.youtube.com/watch?v=tJGYmfTaFRw",
    "https://www.youtube.com/watch?v=T7euc5WZbCw",
    "https://www.youtube.com/watch?v=RiA9PtTLi-Q",
    "https://www.youtube.com/watch?v=XgBnPwklg6U",
    "https://www.youtube.com/watch?v=nIQ2WaZ2R2Y",
    "https://www.youtube.com/watch?v=IgxzcOugvEI",
    "https://www.youtube.com/watch?v=D_1E8BTSVqM",
    ...
    ]

 is what the contents look like.


We will need to use the wordpiece tokenizer from bert model architecture, and
you will need to make a method for trying to group sentences and paragraphs together
so that they have less than 384 wordpiece tokens.
So, we group together n-1 strings of text, (appropriately spacing them with " " between each one),
and then once our text goes over 384 tokens to get n strings of text, we make the paragraph text
the n-1 strings of text (the largest that we can get before reaching 384 tokens).

That is how we shall define our paragraph text. We do that for each video.
While doing this, we shall also include the timestamp as the stamp at the start of the video,
so make timestamp = {start: #:##, end: #:##}
and end is the offset at the n-1'th string.
then also include the youtube url, which can be found from the directory structure in transcripts and youtube url
matching. Do that for us.

Try to also have the paragraph_text end with a full stop. We should make n-1 be the last text of string that is
a full stop at the end before going over 384 wordpiece tokens. Do that for us if possible.


























we will want a topic generator. It would be nice to have around 250-1000 topics. we should do this using
pca/dimensionality reduction and clustering, then finding keywords within cluster, then asking AI to give an appropriate topic name.

Now, we will do the following:













make a pipeline for processing YouTube talks into the appropriate schema and articles.
make a pipeline for processing articles C:\Users\doren\PycharmProjects\ChomskyArchive\web_data\chomsky_articles_20241124_224223.html
make a pipeline for processing debates C:\Users\doren\PycharmProjects\ChomskyArchive\web_data\chomsky_debates_20241124_224231.html
make a pipeline for processing interviews C:\Users\doren\PycharmProjects\ChomskyArchive\web_data\chomsky_interviews_20241124_224229.html
make a pipeline for processing letters C:\Users\doren\PycharmProjects\ChomskyArchive\web_data\chomsky_letters_20241124_224233.html
make a pipeline for processing talks C:\Users\doren\PycharmProjects\ChomskyArchive\web_data\chomsky_talks_20241124_224227.html

