{
  "video_id": "5-uOijZ5mRo",
  "url": "https://www.youtube.com/watch?v=5-uOijZ5mRo",
  "transcript": [
    {
      "text": "So let’s talk about complexity. For a codebreaker\nwho spends all day decrypting messages and",
      "start": 0.25,
      "duration": 5.299
    },
    {
      "text": "unlocking secrets, knowing what kind of cipher\nwas used makes puzzling out the answer a lot easier.",
      "start": 5.549,
      "duration": 5.051
    },
    {
      "text": "But the vast machine of everyday communication\ndidn't come with any blueprints. We need to",
      "start": 10.6,
      "duration": 4.659
    },
    {
      "text": "work extra hard at trying to reverse engineer\nlanguage into its component parts to get out",
      "start": 15.259,
      "duration": 4.261
    },
    {
      "text": "just how complicated a system it really is.\nSo what do we find when we finally crack the",
      "start": 19.52,
      "duration": 4.38
    },
    {
      "text": "code? I’m Moti Lieberman, and this is The\nLing Space.",
      "start": 23.9,
      "duration": 2.6
    },
    {
      "text": "Any language can be broken up into different\nsystems: syntax, semantics, morphology, phonology.",
      "start": 40.329,
      "duration": 4.861
    },
    {
      "text": "And different languages divvy up the workload\nin different ways. But linguists generally",
      "start": 45.19,
      "duration": 4.389
    },
    {
      "text": "agree that, on the whole, all languages are\nequally capable of expressing pretty complicated",
      "start": 49.579,
      "duration": 4.55
    },
    {
      "text": "ideas. But what about human language overall?\nJust how complex is it, and how do we even",
      "start": 54.129,
      "duration": 4.98
    },
    {
      "text": "measure that? To get to the bottom of things,\nwe’ll need to hone in on what language actually is.",
      "start": 59.109,
      "duration": 4.691
    },
    {
      "text": "As we’ve touched on before, one thing that\nwe can rule out pretty much right away is",
      "start": 63.89,
      "duration": 3.78
    },
    {
      "text": "the idea that a language is just a bunch of\nsentences, pre-built and ready to be pronounced.",
      "start": 67.67,
      "duration": 4.69
    },
    {
      "text": "If that were true, each of us would need an\ninfinite memory to carry them all around in",
      "start": 72.36,
      "duration": 3.61
    },
    {
      "text": "our heads. That’s because, in principle,\nthere’s no upper limit on how many sentences",
      "start": 75.97,
      "duration": 4.469
    },
    {
      "text": "there are, or even how long each one can be!\nLike, take the sentence “Joan should be",
      "start": 80.439,
      "duration": 4.601
    },
    {
      "text": "allowed to work at Bletchley Park”.",
      "start": 85.04,
      "duration": 1.96
    },
    {
      "text": "We can always add a few words to the beginning\nof it and still end up with a perfectly good",
      "start": 87.0,
      "duration": 3.97
    },
    {
      "text": "sentence, like in “Alan thinks that Joan\nshould be allowed to work at Bletchley Park”.",
      "start": 90.97,
      "duration": 4.32
    },
    {
      "text": "And there's nothing stopping us from doing that again. And again. And again.",
      "start": 95.29,
      "duration": 3.719
    },
    {
      "text": "So, it doesn’t seem very likely that we’ve\njust got all these possible sentences memorized",
      "start": 99.009,
      "duration": 4.781
    },
    {
      "text": "ad infinitum, especially since you’ve probably\nnever even heard most of them before, but",
      "start": 103.79,
      "duration": 4.1
    },
    {
      "text": "you still recognize them as English. Instead\nof a list, language is more like a set of",
      "start": 107.89,
      "duration": 4.25
    },
    {
      "text": "rules encoded in our heads, which we can tap\ninto at any moment, to produce any sort of",
      "start": 112.14,
      "duration": 4.22
    },
    {
      "text": "sentence we like!",
      "start": 116.36,
      "duration": 1.34
    },
    {
      "text": "And we’ve seen these rules in action before.\nWhen we talked about X-bar theory, we showed",
      "start": 117.7,
      "duration": 3.94
    },
    {
      "text": "you the sorts of trees that we use to represent\nthe structures of sentences. And at the roots",
      "start": 121.64,
      "duration": 4.079
    },
    {
      "text": "of these trees are the rules that generate\nthem. Take a sentence like “Christopher",
      "start": 125.719,
      "duration": 3.74
    },
    {
      "text": "might help”, which looks like this.",
      "start": 129.459,
      "duration": 1.701
    },
    {
      "text": "Underneath this lies a couple of rules, which\ncan be put together and fed into each other.",
      "start": 131.16,
      "duration": 4.53
    },
    {
      "text": "These say that a sentence — or inflectional\nphrase — can be made up out of a noun phrase,",
      "start": 135.69,
      "duration": 3.82
    },
    {
      "text": "plus some indication of tense or modality,\nplus a verb phrase.",
      "start": 139.51,
      "duration": 3.339
    },
    {
      "text": "There’s a lot of different kinds of rules\nwe can come up with to show how elements come",
      "start": 142.849,
      "duration": 3.961
    },
    {
      "text": "together, different combinations that encode\nany linguistic system. But how complex do",
      "start": 146.81,
      "duration": 4.53
    },
    {
      "text": "the rules need to be to fully capture actual\nhuman language? Depending on the answer, we",
      "start": 151.34,
      "duration": 4.59
    },
    {
      "text": "can get an idea of how complex natural language\nreally is compared to other systems, like",
      "start": 155.93,
      "duration": 4.47
    },
    {
      "text": "the artificial languages that we program computers\nwith. That’s because once we can nail down",
      "start": 160.4,
      "duration": 4.38
    },
    {
      "text": "the kinds of rules we need to account for\nit, we can fit them into the Chomsky hierarchy,",
      "start": 164.78,
      "duration": 4.3
    },
    {
      "text": "named after linguist Noam Chomsky.",
      "start": 169.08,
      "duration": 1.75
    },
    {
      "text": "The Chomsky hierarchy is like a ranking of\ndifferent types of rules, along with the different",
      "start": 170.83,
      "duration": 4.04
    },
    {
      "text": "kinds of languages they’re able to generate.\nThe rules at the very bottom of the hierarchy",
      "start": 174.87,
      "duration": 4.089
    },
    {
      "text": "form what are called Type-3 grammars, with\nmore sophisticated rules showing up the higher",
      "start": 178.959,
      "duration": 3.821
    },
    {
      "text": "you go on the list, all the way up to Type-0.\nIt’s like golf, where smaller numbers score higher.",
      "start": 182.78,
      "duration": 4.82
    },
    {
      "text": "So where does natural language fit? Well,\nlet’s start with the simplest possible grammars,",
      "start": 187.65,
      "duration": 4.8
    },
    {
      "text": "the Type-3s, and see if it fits there. These\nwork using rules that look a bit like this.",
      "start": 192.45,
      "duration": 4.759
    },
    {
      "text": "That first rule works by outputting some word\nin the language you want to generate, followed",
      "start": 197.209,
      "duration": 3.961
    },
    {
      "text": "by a placeholder which can be filled in with\nmore words later on. So this rule could start",
      "start": 201.17,
      "duration": 4.349
    },
    {
      "text": "by producing something like “If phrase”.\nThen, we can apply the rule again to replace",
      "start": 205.519,
      "duration": 4.36
    },
    {
      "text": "that phrase part with another word, followed\nby yet another phrase; so, we get “If you",
      "start": 209.879,
      "duration": 4.39
    },
    {
      "text": "phrase”, “If you pass phrase”, and so\non.",
      "start": 214.269,
      "duration": 3.241
    },
    {
      "text": "We can keep going like this as long as we\nlike, until we get to the end of our sentence",
      "start": 217.51,
      "duration": 3.899
    },
    {
      "text": "and apply our second rule. It says we can\nfill in that last bit with just one word,",
      "start": 221.409,
      "duration": 4.97
    },
    {
      "text": "so we can get “If you pass the Turing test,\nthen you’re conscious”.",
      "start": 226.379,
      "duration": 2.921
    },
    {
      "text": "The sort of language generated by a Type-3\ngrammar is known as a regular, or finite",
      "start": 229.3,
      "duration": 4.659
    },
    {
      "text": "state, language. Since we managed to get\nour rule to produce an English sentence, maybe",
      "start": 233.959,
      "duration": 3.95
    },
    {
      "text": "English is a finite state language!",
      "start": 237.909,
      "duration": 2.0
    },
    {
      "text": "But as you probably guessed, it isn’t quite\nthat simple. For one, the number of \"if\"s in",
      "start": 239.909,
      "duration": 4.071
    },
    {
      "text": "a sentence like that usually has to match\nthe number of \"then\"s. So, you can’t have",
      "start": 243.98,
      "duration": 3.869
    },
    {
      "text": "sentences like “If if you pass the Turing\ntest, then you’re conscious” or “You",
      "start": 247.849,
      "duration": 4.801
    },
    {
      "text": "pass the Turing test, then then then you’re\nconscious”. And our rules are too simple",
      "start": 252.65,
      "duration": 4.549
    },
    {
      "text": "to account for that - they’ll let you stack\nthings up wrong, or give you garbled results.",
      "start": 257.199,
      "duration": 4.401
    },
    {
      "text": "Now, there is a bit of flexibility here, and\nwith the right amount of tinkering, we could",
      "start": 261.6,
      "duration": 4.14
    },
    {
      "text": "probably debug our rules and sidestep these\nspecific cases. But the problem only gets worse.",
      "start": 265.74,
      "duration": 4.96
    },
    {
      "text": "Since any clause in a finite state grammar can be replaced with any other clause of the",
      "start": 270.7,
      "duration": 4.1
    },
    {
      "text": "same sort, we can replace the first half of\nthat sentence with something like “either",
      "start": 274.85,
      "duration": 4.02
    },
    {
      "text": "you pass the Turing test, or you have a mind”,\nso the end result winds up as “If either",
      "start": 278.87,
      "duration": 4.62
    },
    {
      "text": "you pass the Turing test or you have a mind,\nthen you’re conscious”.",
      "start": 283.49,
      "duration": 3.239
    },
    {
      "text": "And again, nothing’s stopping us at just\none swap, so we can also get a sentence like",
      "start": 286.729,
      "duration": 4.351
    },
    {
      "text": "“If either you pass either the Turing test\nor a psychological exam, or you have a mind,",
      "start": 291.08,
      "duration": 4.97
    },
    {
      "text": "then you’re conscious”. In general, you\nneed as many ors as there are eithers; there’s",
      "start": 296.05,
      "duration": 3.98
    },
    {
      "text": "really no room for error.",
      "start": 300.03,
      "duration": 1.57
    },
    {
      "text": "This is working so far, with a lot of effort,\nbut it looks like we really need some kind",
      "start": 301.6,
      "duration": 4.68
    },
    {
      "text": "of counter to keep track of how many \"if\"s and\n\"either\"s we use, or the rules might just start giving",
      "start": 306.28,
      "duration": 4.07
    },
    {
      "text": "us nonsense. Our grammar needs some kind of\nmemory, which a Type-3 grammar just doesn’t have.",
      "start": 310.35,
      "duration": 5.15
    },
    {
      "text": "So, what about the next level up — Type-2?\nWell, these grammars give us a lot more flexibility.",
      "start": 315.52,
      "duration": 5.75
    },
    {
      "text": "Some of the many rules allowed by a Type-2\ngrammar look like this.",
      "start": 321.27,
      "duration": 3.56
    },
    {
      "text": "This new set-up looks a lot like our old one,\njust with a couple of extra rules that make sure",
      "start": 324.83,
      "duration": 4.209
    },
    {
      "text": "our sentences don’t run amok with too many\nifs or too few ors. With just a couple of extra,",
      "start": 329.039,
      "duration": 4.591
    },
    {
      "text": "more powerful rules, we’re able keep these\npairs consistently matched up. And we can",
      "start": 333.63,
      "duration": 4.509
    },
    {
      "text": "get a good sense of just how complex language\nhas to be, too, since now we can compare it",
      "start": 338.139,
      "duration": 4.241
    },
    {
      "text": "to other Type-2 languages! For instance, the\nartificial language of sentential logic, which",
      "start": 342.38,
      "duration": 4.259
    },
    {
      "text": "we talked about a while back, falls into this\nclass, along with most computer programming",
      "start": 346.639,
      "duration": 4.131
    },
    {
      "text": "languages. And our X-bar rule for sentences\ndoes too! So we’re all done! Right?",
      "start": 350.77,
      "duration": 5.0
    },
    {
      "text": "Well… not so fast. As it happens, there\nare patterns in language that can’t be generated",
      "start": 355.77,
      "duration": 4.61
    },
    {
      "text": "by these rules, either. Take a sentence like\n“Charles says that we helped John paint",
      "start": 360.38,
      "duration": 4.409
    },
    {
      "text": "the house”. Like we saw before, we can keep\nextending this sentence indefinitely, so “Charles",
      "start": 364.789,
      "duration": 5.47
    },
    {
      "text": "says that we let the children help John paint\nthe house”, “Charles says that we let",
      "start": 370.259,
      "duration": 3.88
    },
    {
      "text": "Mary let the children help Peter help John\npaint the house”, and so on. It’s not",
      "start": 374.139,
      "duration": 4.011
    },
    {
      "text": "a problem for English, since our existing\nrules are more than capable of handling it.",
      "start": 378.15,
      "duration": 4.049
    },
    {
      "text": "But when we translate these sentences into\na dialect of Swiss German, things get tricky.",
      "start": 382.199,
      "duration": 5.041
    },
    {
      "text": "For one, the verbs “let” and “help”\neach need their own kind of specially marked",
      "start": 387.24,
      "duration": 3.899
    },
    {
      "text": "noun phrase — an accusative one, in the\ncase of “let”, and a dative one, in the",
      "start": 391.139,
      "duration": 3.56
    },
    {
      "text": "case of “help”. Overall, there need to\nbe as many accusative nouns as there are lets,",
      "start": 394.699,
      "duration": 4.681
    },
    {
      "text": "and as many dative nouns as there are helps.\nAnd there’s no upper limit to how many lets",
      "start": 399.38,
      "duration": 4.24
    },
    {
      "text": "and helps there can be.",
      "start": 403.62,
      "duration": 1.669
    },
    {
      "text": "What really throws a wrench into the works,\nthough, is that in Swiss German, all of the",
      "start": 405.289,
      "duration": 3.961
    },
    {
      "text": "nouns actually get grouped together at the\nfront of the embedded clause, before all the verbs.",
      "start": 409.25,
      "duration": 4.65
    },
    {
      "text": "To visualize what’s going on, you can think\nof the overall pattern like this: sandwiched",
      "start": 413.97,
      "duration": 4.74
    },
    {
      "text": "in between the beginnings and ends of each\nof these sentences, we always find some number",
      "start": 418.71,
      "duration": 4.12
    },
    {
      "text": "of accusative nouns, followed by some other\nnumber of dative nouns, followed by however",
      "start": 422.83,
      "duration": 4.179
    },
    {
      "text": "many lets you need, followed by however many\nhelps.",
      "start": 427.009,
      "duration": 2.851
    },
    {
      "text": "And because of this bunching together of all\nthe nouns and all the verbs, when we draw",
      "start": 429.86,
      "duration": 4.16
    },
    {
      "text": "lines connecting each noun to each verb, those\nlines cross. For this reason, Swiss German",
      "start": 434.02,
      "duration": 5.17
    },
    {
      "text": "is said to have crossing dependencies. And\nthat’s a real problem, since it can be mathematically",
      "start": 439.19,
      "duration": 5.259
    },
    {
      "text": "proven that this kind of language can’t\nbe generated by a Type-2 grammar - check the",
      "start": 444.449,
      "duration": 4.381
    },
    {
      "text": "video description if you want to know how.",
      "start": 448.83,
      "duration": 1.839
    },
    {
      "text": "Because of that, we’ve concluded that natural\nlanguage is at least mildly context-sensitive.",
      "start": 450.669,
      "duration": 4.43
    },
    {
      "text": "The rules of the next grammar up the list,\nso Type-1, can reference contexts, and they",
      "start": 455.099,
      "duration": 4.561
    },
    {
      "text": "generally look like this:",
      "start": 459.66,
      "duration": 1.81
    },
    {
      "text": "And we’ve actually seen these kinds of rules\nalready. When Chomsky was originally developing",
      "start": 461.47,
      "duration": 4.069
    },
    {
      "text": "his theory of grammar, he proposed adding\ncontext-sensitive transformations to solve",
      "start": 465.539,
      "duration": 4.35
    },
    {
      "text": "the problem posed here. In modern linguistics,\nthis has taken the form of syntactic movement —",
      "start": 469.889,
      "duration": 4.751
    },
    {
      "text": "something we now see just about everywhere,\nfrom question formation to raising verbs,",
      "start": 474.64,
      "duration": 4.249
    },
    {
      "text": "and more! So since natural language needs\nthe type of rules that only Type-1 systems",
      "start": 478.889,
      "duration": 4.631
    },
    {
      "text": "or higher have, we know that’s what\nwe want.",
      "start": 483.52,
      "duration": 3.0
    },
    {
      "text": "But what about Type-0? Well, as it turns out,\nType-0 grammars end up being just too needlessly",
      "start": 486.52,
      "duration": 5.619
    },
    {
      "text": "powerful to apply to human language, like\nusing a supercomputer to do your taxes. Type-0",
      "start": 492.139,
      "duration": 5.601
    },
    {
      "text": "grammars have rules which are “unrestricted”,\nwhich means that you can have any combination",
      "start": 497.74,
      "duration": 3.5
    },
    {
      "text": "of symbols to the left of the arrow, and any\ncombination of symbols on the right. There’s",
      "start": 501.24,
      "duration": 4.01
    },
    {
      "text": "no real template for these sorts of rules,\nexcept something like “A→B”, where “A”",
      "start": 505.25,
      "duration": 3.9
    },
    {
      "text": "and “B” can be literally anything. So\nalthough we can make machines talk to each",
      "start": 509.15,
      "duration": 4.78
    },
    {
      "text": "other with unrestricted grammars, when you\nline that up with what we know of natural languages,",
      "start": 513.93,
      "duration": 4.109
    },
    {
      "text": "it just doesn’t compute. That’s why Type-1\nhas just the right degree of complexity.",
      "start": 518.039,
      "duration": 5.06
    },
    {
      "text": "So, we’ve reached the end of The Ling Space\nfor this week. If you solved the enigma of",
      "start": 523.099,
      "duration": 4.581
    },
    {
      "text": "language, you learned that we can think of\na language like a set of rules that come together",
      "start": 527.68,
      "duration": 3.839
    },
    {
      "text": "to generate all of its sentences; that we\ncan rank those rules, using the Chomsky hierarchy;",
      "start": 531.519,
      "duration": 4.901
    },
    {
      "text": "and that human language lands somewhere in\nthe upper half, in terms of power and sophistication.",
      "start": 536.42,
      "duration": 4.27
    },
    {
      "text": "The Ling Space is produced by me, Moti Lieberman,\nand directed by Adèle-Elise Prévost. This",
      "start": 540.69,
      "duration": 4.71
    },
    {
      "text": "week’s episode was written by Stephan Hurtubise.\nOur editor is Georges Coulombe, our music",
      "start": 545.4,
      "duration": 4.07
    },
    {
      "text": "is by Shane Turner, and our graphics team\nis atelierMUSE. We’re down in the comments",
      "start": 549.47,
      "duration": 3.85
    },
    {
      "text": "below, or you can bring the discussion back\nover to our website, where we’ll have some",
      "start": 553.32,
      "duration": 3.49
    },
    {
      "text": "extra material on this topic.",
      "start": 556.81,
      "duration": 1.81
    },
    {
      "text": "Check us out on Tumblr, Twitter, and Facebook,\nand try dropping by our store. And if you",
      "start": 558.62,
      "duration": 3.94
    },
    {
      "text": "want to keep expanding your own personal Ling\nSpace, please subscribe. And we’ll see you",
      "start": 562.56,
      "duration": 4.02
    },
    {
      "text": "next Wednesday. Vidimo se uskoro!",
      "start": 566.58,
      "duration": 2.52
    }
  ]
}